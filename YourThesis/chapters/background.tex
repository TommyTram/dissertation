% !TEX root=../../Thesis.tex
\chapter{Technical background}\label{chapter:background}
%

\section{Markov decision process}\label{sec:mdp}

\begin{itemize}
    \item POMDP
    \item State - realative features, scalable to fidderent types of intersection design 
    \item Action - IDM - MPC
    \item Observation - unknown intentions, red lights, stop signs yield. 
    \item Transistion function - RL 
    \item Reward Function - Goal, Comfort, crash
    \item discount factor 
\end{itemize}


% Now not using time index, e.g., s_t. Check if consistent with subsequent chapters.
%    - Time index not used (much) in Paper 2-5, but in Paper 6. Slighlty inconsistent at the moment, but doesn't matter too much...

% This chapter gives a brief introduction to
% %how the problem of making sequential decisions based on uncertain information can be modeled, 
% Markov decisions processes
% and reinforcement learning. Notation that is used in subsequent chapters is also introduced. The material in this chapter is based on Kochenderfer~\cite{Kochenderfer2015} and Sutton et al.~\cite{Sutton2018}, which provide a comprehensive overview of MDPs and RL.
This chapter briefly introduce the \gls{pomdp} framework and reinforcement learning.  


\subsection{Partially observable Markov decision process}\label{section:pomdp}
A \gls{pomdp} is a mathematical framework used for modeling sequential decision making problems under uncertainty. 
It is a generalization of the \gls{mdp}
It consists of a set of states $\mathcal{S}$, actions $\mathcal{A}$, observations $\Omega$, a transition model $T$, observation model $O$, reward function $R$, and a discount factor $\gamma$. 
Together they create the tuple $(\mathcal{S},\mathcal{A},\Omega,T,O,R,\gamma)$ that formally defines a \gls{pomdp}~\cite{Kochenderfer2015}. 
While in a state $s \in \mathcal{S}$ and executing an action $a \in \mathcal{A}$ the probability of transitioning to a future state $s^\prime$ is described by the transition model $T(s^\prime \mid s,a)$ and the reward $r$ is given by the reward function $R(s,a)$. 
The agent only has access to partial information contained in its observation $o$ distributed according to $\Pr(o \mid s', a) = O(o, s', a)$.

\tommy{CJ}
This chapter provides a brief introduction to Markov decision processes and reinforcement learning.
%, together with the notation that is used in subsequent chapters. 
% The reader who is familiar with these concepts could probably skip this chapter and return in case of any confusion concerning the notation. 
The purpose of the chapter is to summarize the most important concepts and introduce the notation that are used in the subsequent chapters. 
A comprehensive overview of MDPs and RL is given in the books by Kochenderfer~\cite{Kochenderfer2015} and Sutton and Barto~\cite{Sutton2018}, upon which this chapter is based.

\section{Reinforcement learning}


\tommy{CJ: Markov decision processes}

%MDP
Sequential decision-making problems in stochastic environments are commonly modeled as MDPs. Importantly, an MDP satisfies the Markov property, which requires that the probability distribution of the next state only depends on the current state and the action taken by the agent, i.e., not on the history of previous states or actions. An MDP is formally defined as the tuple $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$, which is described in the following list~\cite[Ch. 4]{Kochenderfer2015}:
%
\begin{itemize}
    \item The state space $\mathcal{S}$ represents the set of all possible states of the environment. This set could consist of both discrete and continuous states.
    \item The action space $\mathcal{A}$ represents the set of all valid actions the agent can take. This set could also consist of both discrete and continuous actions. However, since this thesis focuses on high-level decision-making, only discrete actions are considered here.
    \item The state transition model $T(s'|s,a)$ describes the probability that the system transitions to state $s' \in \mathcal{S}$ from state $s \in \mathcal{S}$ when action $a \in \mathcal{A}$ is taken.
    \item The reward function $R(s,a)$ returns a scalar reward $r$ for each state-action pair.
    \item The discount factor $\gamma \in [0,1)$ is a scalar that discounts the value of future rewards. For a finite horizon MDP, $\gamma$ could also take the value~$1$.
\end{itemize}

A policy $\pi$ is a mapping from a state to an action, which could either be deterministic $a=\pi(s)$ or probabilistic $a \sim \pi(a|s)$. The value of being in a state while following a policy is described by the value function
%
\begin{align}
    V^\pi(s) = \mathbb{E} \left[ \sum_{k=0}^\infty \gamma^k R(s_k, a_k) | s_0 = s, \pi \right].
\end{align}
%
The goal of the agent is to find a policy which maximizes the value of each state.

% In this framework, an agent chooses an action $a$, based on the current state $s$, then receives a reward $r$, and transitions to a new state $s'$. An MDP satisfies the Markov property, which assumes that the probability distribution of the next state only depends on the current state and action, and not on the history of previous states. The MDP is defined as the tuple $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $T$ is a state transition model, $R$ is a reward model, and $\gamma \in [0,1]$ is a discount factor. 
% The state transition model $T(s' \mid s,a)$ describes the probability that the system transitions to state $s'$ from state $s$ when action $a$ is taken, and the reward model defines the reward of each step as $r=R(s,a,s')$.
% The goal of an agent is to find a policy $\pi(s)$ which for every time step $t$ chooses an action that maximizes the future discounted return $R_t$, defined as
% %
% \begin{align}
%     \label{eq:discountedReturn}
%     R_t = \sum_{k=0}^\infty \gamma^k r_{t+k},
% \end{align}
% %
% where $r_{t+k}$ is the reward at step $t+k$.


%POMDP
In many decision-making problems, the agent does not have direct access to the state of the environment. Such a problem is commonly modeled as a partially observable Markov decision process, which is an extension to the MDP framework that also models state uncertainty. A POMDP is formally defined as the tuple  $( \mathcal{S}, \mathcal{A}, \mathcal{O}, T, O, R, \gamma )$, where the state space, action space, transition model, reward function, and discount factor are defined as for an MDP. A POMDP has two additional components~\cite[Ch. 6]{Kochenderfer2015}:
%
\begin{itemize}
    \item The observation space $\mathcal{O}$, which is the set of possible observations.
    \item The observation model $O(o|s',a)$, which describes the probability of observing $o \in \mathcal{O}$ in state $s'$ after action $a$ has been taken.
\end{itemize}
%
Since the agent does not have direct access to the current state in a POMDP, the agent needs to reason about the history of observations and actions. This history is often merged in a belief state $b$, which represents a probability distribution over the state space. In this case, the policy is a mapping from beliefs to actions $\pi(b)$.



%In many decision-making problems, the exact state is not known by the agent and it only perceives observations $o$. A problem with state uncertainty can be modeled as a partially observable Markov decision process, which is defined by the tuple $( \mathcal{S}, \mathcal{A}, \mathcal{O}, T, O, R, \gamma )$. Compared to an MDP, the POMDP includes an additional observation space $\mathcal{O}$, and an observation model $O(o \mid s,a,s')$, which describes the probability of observing $o$ in state $s'$, after taking action $a$ in state $s$.


For many real-world problems, it is not possible to represent the probability distributions $T$ or $O$ explicitly. For some solution approaches, only samples are needed, and then it is sufficient to define a generative model $G$ that samples a new state or observation from a given state and action, i.e., $s' \sim G(s,a)$ for the MDP case~\cite[Ch. 4]{Kochenderfer2015} and $(s', o) \sim G(s,a)$ for the POMDP case~\cite[Ch. 6]{Kochenderfer2015}.


\tommy{Reinforcement learning}


If all the elements $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$ of an MDP are known, an agent can use this model to directly compute an optimal policy. Such a problem is often considered a planning problem. For small MDPs, dynamic programming\footnote{Dynamic programming refers to simplifying a complex problem by breaking it down into smaller sub-problems, often in a recursive manner.} techniques can provide an exact solution, which is calculated offline, i.e., before the agent is deployed in the environment. For example, in value iteration~\cite[Ch. 4]{Kochenderfer2015}, the Bellman operator is iteratively applied to the value function for all states,
%
\begin{align}
    V_{n+1}(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'}T(s'|,a,s)V_n(s') \right].
\end{align}
%
As $n$ goes to infinity, $V_n$ converges to the unique optimal value function $V^*$, and an optimal policy (not necessarily unique) is extracted by
%
\begin{align}
    \pi(s) = \argmax_a \left[ R(s,a) + \gamma \sum_{s'}T(s'|,a,s)V^*(s') \right].
\end{align}
%
However, for many real-world problems with high dimensional state spaces, it is intractable to compute and store a policy offline. Contrarily to offline methods, online search methods perform planning from the current state up to some horizon, when the agent has been deployed. Thereby, the agent can limit the computation to states that are reachable from the current state, which is often significantly smaller than the full state space~\cite[Ch. 4]{Kochenderfer2015}.

% \begin{figure}[!bt]
%     \centering
%         \vspace{-10pt} % To avoid widow word.
%         \includegraphics[width=0.7\columnwidth]{figures/RL_cropped.pdf}
%         \caption{In a reinforcement learning problem, an agent learns a policy $\pi$ by interacting with its environment. The agent collects experience by repeatedly taking actions and then observing the resulting state and reward.}
%         \vspace{-10pt} % To avoid widow word.
%     \label{fig:RL}
% \end{figure}

In many problems, the state transition probabilities or the reward function are not known. These problems can be solved by reinforcement learning techniques, in which the agent learns how to behave from interacting with the environment~\cite[Ch. 5]{Kochenderfer2015}, see Figure~\ref{fig:RL}. Compared to supervised learning, reinforcement learning presents some additional challenges. Since the data that are available to an RL-agent depends on its current policy, the agent must balance exploring the environment and exploiting the knowledge it has already gained. Furthermore, a reward that the agent receives may depend on a crucial decision that was taken earlier in time, which makes it important to assign rewards to the correct decisions.

%If MDP is known, planning. RL deals with unknown T or G, interacts with environment to learn. Figure.
% As mentioned above, the goal of a decision-making agent is to take actions that maximizes the future discounted return $R_t$. If all elements of the MDP are known, the agent could compute which action that is ideal before executing any actions in the environment, which is referred to as planning. However, in many problems the transition model or generative model is not known to the agent beforehand and it needs to learn how to behave from experience, which is referred to as a reinforcement learning problem. The agent will then act in the environment and observe what happens, in order to figure out a policy $\pi$, which defines which action to take in a given state.
% Figure~\ref{fig:RL} shows a schematic overview of the reinforcement learning problem.
% The agent is commonly represented by a neural network, which acts as a nonlinear function approximator. Further details on neural networks and how they can be used in RL are described by Sutton et al.~\cite{Sutton2018}.


%Reinforcement learning is a class of machine learning techniques, which enables an agent to learn a policy $\pi$ that maximizes the expected future return $\mathbb{E}(R_t)$ in the environment the agent acts in~\cite{Sutton1998}. 



%Model based vs model-free
% Model based - learns model T, then planning
% Model free - does not learn model
% Model free techniques range from direct policy search vs value-based RL

%One approach to decide which action to take is to first try to learn the model of the environment from the observations, i.e., to learn $T$. Once the model is learned, a planning algorithm could be used to find the best policy. RL approaches that follows this structure are called model based RL algorithms.

RL algorithms can be divided into model-based and model-free approaches~\cite[Ch. 5]{Kochenderfer2015}. In the model-based versions, the agent first tries to estimate a representation of the state transition function $T$ and then use a planning algorithm to find a policy. On the contrary, as the name suggests, model-free RL algorithms do not explicitly construct a model of the environment to decide which actions to take. 
The model-free approaches can be further divided into value-based and policy-based techniques. Value-based algorithms, such as $Q$-learning, aim to learn the value of each state and thereby implicitly define a policy. Policy-based techniques instead search for the optimal policy directly in the policy space, either by policy gradient methods or gradient-free methods, such as evolutionary optimization. There are also hybrid techniques that are both policy and value-based, such as actor critic methods.


% Genetic algorithms belong to a family of optimization methods that are inspired by the evolutionary mechanisms of natural selection~\cite{Holland1975}.
% In general, GAs are suitable for solving optimization problems where the objective function is non-differentiable, or even when an explicit mathematical model does not exist and only a simulation is available. 
% A GA can also solve some versions of RL problems, and is used as an RL method in this thesis.


RL algorithms generally assume that the environment is modeled as an MDP, i.e., that the state of the environment is known by the agent. However, in many cases of interest, only partial information about the state of the environment is available, which is modeled in the POMDP framework. For such cases, it is common to approximate the state by either the observation or a finite history of observations~\cite[Ch. 17]{Sutton2018}. The latter is referred to as a $k$-Markov approximation, where $k$ defines the length of the included history. For a sufficiently long history, the Markov property is assumed to approximately hold, even though the environment is partially observable.

% Possibly add something about neural networks as function approximators somewhere in this section?
%    - No
