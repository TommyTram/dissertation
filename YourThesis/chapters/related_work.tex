% !TEX root=../../Thesis.tex
\chapter{Related work}\label{chapter:related_work}

\todo{Urban challange, winner Carnegie Mellon University John Dolan}
\todo{Rule based methods}
\todo{Motion planning, Predicting motion of surrounding vehicles, reactive (not intersactive)}
MPC cite ivo:
A robust scenario MPC approach for uncertain multi-modal obstacles,
Real-Time Constrained Trajectory Planning and Vehicle Control for Proactive Autonomous Driving With Road Users.

\todo{POMDP model, online and offline solvers}
Cite Maxime: 
Cooperation-aware reinforcement learning for merging in dense traffic, 
Belief state planning for autonomously navigating urban intersections.
\todo{online solvers, MCTS}
MCTS cite CJ:
Combining planning and deep reinforcement learning in tactical decision making for autonomous driving,
Tactical decision-making in autonomous driving by reinforcement learning with uncertainty estimation.

\todo{cite Bo wahlberg, Morteza Haghir Chehreghani, Fernandez Llorca David, Christian Berge}

\tommy{rewrite text}
%Rule based methods
Early approaches to tactical decision-making for autonomous vehicles often used rule-based methods, commonly implemented as handcrafted state machines. For example, during the DARPA Urban Challenge, a rule-based approach was adopted by the winning team from the Carnegie Mellon University, where different modules handled the behavior for the different driving scenarios that were encountered~\cite{darpaCMU}. 
% Other participants, such as Stanford and Virginia Tech, used similar strategies~\cite{Bacha2008, darpaStanford}. 
While successful for a limited and controlled environment such as the Urban Challenge event, it is unlikely that rule-based approaches could scale to handle the complexity and diversity of real-world driving.

%Motion planning, predicting motion of surrounding vehicles, reactive (not interactive)
Another group of algorithms treats the decision-making task as a motion planning problem. Commonly, a prediction model is used to predict the motion of the other agents, and then the behavior of the vehicle that is being controlled, henceforth referred to as the ego vehicle, is planned accordingly. This results in a reactive behavior, since the predictions are independent of the ego vehicle plan. Therefore, interaction between the ego vehicle and other agents is not explicitly considered, but may happen implicitly by frequent replanning.
Search-based planners typically discretize the state space and then apply Dijkstra's algorithm~\cite{Dijkstra1959} or one of the algorithms from the A* family~\cite{Hart1968}. These techniques were also commonly used during the DARPA Urban Challenge~\cite{Bacha2008, darpaStanford}. However, since real-time performance can be hard to achieve with graph-search algorithms, sampling-based algorithms such as rapidly-exploring random trees~\cite{Lavalle1998} have been used for motion planning, e.g., by Karaman et al.~\cite{Karaman2011}. 
A third approach to solve the motion planning problem is to use optimization-based techniques, for example optimal control, which was applied to highway driving scenarios by Werling et al.~\cite{Werling2010}. Since human behavior is complex and varies between individuals, some algorithms use a probabilistic prediction as input to the motion planning. This is for example shown in a study by Damerow et al.~\cite{Damerow2015}, which aims to minimize the risk during an intersection scenario.
Additional approaches to motion planning for autonomous driving are provided in the surveys by Gonz√°les et al.~\cite{Gonzales2016} and Paden et al.~\cite{Paden2016}. 


%POMDP model, online and offline solvers
It is common to model decision-making problems as Markov decision processes or partially observable Markov decision processes (POMDPs)~\cite{Kochenderfer2015}. This mathematical framework allows modeling of uncertainty in the current state, uncertainty in the future development of the traffic scene, and modeling of an interactive behavior. The task of finding the optimal policy for a POMDP is most often intractable, but many approximate methods exist. One way to group these methods is in offline and online methods. There are powerful offline algorithms for planning in POMDPs, which can solve complex situations. One example is shown by Brechtel et al., which proposes a solution to how measurement uncertainty and occlusions in an intersection can be handled~\cite{Brechtel2014}. In their work, an offline planner precomputes the policy by using a state representation that is learned for the specific scenario. A similar approach is adopted by Bai et al.\ for an intersection scenario~\cite{Bai2014}. The main drawback of these offline methods is that they are designed for specific scenarios. Due to the large number of possible real-world scenarios, it is challenging to precalculate a policy that is generally valid.


%Online solvers
Online methods compute a policy during execution, which makes them more versatile than offline methods. However, the limited available computational resources require a careful problem formulation and limit the solution quality.
% Ulbrich et al.\ use a POMDP framework to make decisions on when to change lanes during highway driving~\cite{Ulbrich2015}. In order to make it possible to solve the problem with an exhaustive search, a problem-specific high-level state space is created, which consists of states that represent whether or not a lane change is possible or beneficial. However, due to the specialized state space, it is hard to generalize this method.
Another online approach for solving a POMDP is the family of Monte Carlo tree search algorithms~\cite{Browne2012}, which is used by Sunberg et al.\ to make lane changing decisions on a highway~\cite{Sunberg2017}. 
% In order to handle the continuous state description, the tree search is extended with a technique called progressive widening~\cite{Couetoux2011}. 
Furthermore, other drivers' intentions are estimated with a particle filter. %\cite{Lenz2016} also applied a MCTS method for lane change decisions.
A hybrid approach between offline and online planning is pursued in a study by Sonu et al., where a hierarchical decision-making structure is used~\cite{Sonu2018}. The decision-making problem is modeled on two levels as MDPs, since full observability is assumed. The high-level MDP is solved offline by value iteration and the low-level MDP is solved online with MCTS.
% Requires transition model, hard to obtain.


\vspace{8pt}

\section{Learning-based methods}
\tommy{rewrite}

In planning-based methods, different algorithms are used to find a policy that maximizes the utility of the agent's behavior by using a model of the system. However, data-driven approaches are fundamentally different, since with this class of methods, an agent instead learns how to behave from observing data. This section describes different types of data-driven approaches that have been explored for autonomous driving.

% Imitation learning, behavioral cloning
An intuitive approach is to collect data from when an expert is performing a task and then use supervised learning to imitate the behavior of the expert. This method is often referred to as behavioral cloning and was first applied to autonomous driving in the ALVINN project~\cite{Pomerleau1989}. Unfortunately, for many cases, behavioral cloning suffers from compounding errors, which refers to the problem when small mistakes gradually push the agent further away from the training distribution, into states from which the agent does not know how to recover. This problem can be mitigated by an active learning approach, where an expert can be queried during the training process~\cite{Ross2011}, which for example has been applied to autonomous driving by Kelly et al.~\cite{Kelly2019}. An alternative is to synthesize data by perturbing the expert's driving, which was done in a study by Bansal et al.~\cite{Bansal2019}. Generative adversarial imitation learning~\cite{Ho2016} provides another method to handle the compounding error problem and has showed promising results in different highway driving scenarios~\cite{Kuefler2017}.

%RL
Reinforcement learning is conceptually different from supervised learning, since labeled input-output samples are not available. Instead, the agent learns how to make decisions from interacting with the environment.
%Reinforcement learning is conceptually different from supervised learning, which is used for behavioral cloning, since labeled input/output samples are not available in RL problems. Instead, an RL-based agent typically learns how to make decisions from interacting with an environment, in which the agent occasionally receives a reward signal that tells the agent how well it is performing.
RL methods are versatile, and have proven successful in various domains, such as playing Atari games~\cite{Mnih2015}, in continuous control~\cite{Lillicrap2015}, reaching a super human performance in the game of Go~\cite{Silver2017}, and beating the best chess computers~\cite{Silver2017chess}. One advantage of RL methods, compared to planning based methods, is that a model of the environment is not required, i.e., the transition probabilities between different states are not assumed to be known. Furthermore, many RL methods provide a general framework and an agent could, in theory, learn how to behave correctly in all possible driving situations.
%
During the last few years, many papers have addressed the task of applying RL approaches to autonomous driving. For example, DQN-based agents were used by Isele et al.~\cite{Isele2018} for navigating through different intersection scenarios, with varying driver intentions and occlusions. Commonly, a high-level action space is used together with the DQN algorithm. Other studies use policy gradient RL methods to directly control the speed and the steering angle of the vehicle, for example in lane changing and urban scenarios~\cite{Wang2019_ddpg, Chen2019}.
Safety of the RL-based agents has been addressed by restricting dangerous actions, either by heuristics~\cite{Mukadam2017}, linear temporal logic~\cite{Bouton2019}, or using an underlying motion planner with hard constraints~\cite{Shalev2016}.

A majority of these studies perform both the training and evaluation in simulated environments, whereas some train the agent in a simulator and then apply the trained agent in the real world~\cite{Pan2017, Bansal2019}, or for some limited scenarios, the training itself is also performed in the real world~\cite{Kendall2019}. 
Overviews of RL-based studies for autonomous driving are given by Kiran et al.~\cite{Kiran2021} and by Ye et al.~\cite{Ye2021}.
% Add reference to Pin Wang's new review paper. Possibly replace one of the other survey papers?
%    - DONE

Both planning-based and RL-based methods use a reward function to find a policy that maximizes the cumulative future reward. However, for complex tasks such as autonomous driving, the design of the reward function is in itself a complicated task. For limited scenarios, the reward function can be manually specified and tuned until the agent finds a desired behavior, which is referred to as reward shaping~\cite{Ng1999}. However, for realistic scenarios, the number of possible reward features is massive and how to balance rewards related to, e.g., safety and efficiency is a complex issue. A practical approach is to instead learn the reward function from the behavior of human drivers by inverse reinforcement learning (IRL)~\cite{Ng2000}. An IRL approach was for example used by Kuderer et al.\ to learn the individual preferences of human drivers with different driving styles~\cite{Kuderer2015}. Sharifzadeh et al.\ combined IRL with DQN and Wang et al.\ used an adversarial IRL approach to simultaneously obtain both the reward function and the policy for different lane-changing scenarios~\cite{Sharifzadeh2016, Wang2019}. 
Zhu et al.~\cite{Zhu2021} provide an overview of IRL applied to autonomous driving.
Except for performing planning and RL, the learned reward function can also be used to predict the behavior of other traffic participants. IRL for predictions was for example used by Ziebart et al.\ for pedestrians~\cite{Ziebart2009}, by Sun et al.\ for human drivers in intersections~\cite{Sun2019}, and by Sadigh for highway driving situations~\cite{Sadigh2016}.

