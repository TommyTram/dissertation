% !TEX root=../../Thesis.tex
\chapter{Technical background}\label{chapter:background}
This chapter briefly introduce the \gls{pomdp} framework and reinforcement learning. A more comprehensive overview of \gls{pomdp}s and \gls{rl} is given in the books by Kochenderfer~\cite{Kochenderfer2015} and Sutton and Barto~\cite{Sutton2018}, upon which this chapter is based. The purpose of the chapter is to summarize the most important concepts and introduce the notation that are used in the subsequent chapters. 
\todo{
\begin{itemize}
    \item MDP
    \item POMDP
    \item State - realative features, scalable to fidderent types of intersection design 
    \item Action - IDM - MPC
    \item Observation - unknown intentions, red lights, stop signs yield. 
    \item Transistion function - RL 
    \item Reward Function - Goal, Comfort, crash
    \item discount factor 
\end{itemize}
}

\section{Markov decision process}\label{sec:mdp}
A \gls{mdp} is a mathematical framework for modeling discrete time sequential decision making problems. It involves an agent making decisions in an environment evolving over time according to a stochastic process. The state of the environment contains all the information necessary about the agent and environment at a given time to be able to transition to a next state. This property is referred to as the Markov property. 

The \gls{mdp} is formally defines as the tuple $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$, described by the following list~\cite{Kochenderfer2015}:
\begin{itemize}
    \item The state space $\mathcal{S}$ represents the set of all possible states of the environment. This set could consist of both discrete and continuous states.
    \item The action space $\mathcal{A}$ represents the set of all possible actions the agent can take. The action space can  consist of both discrete and continuous actions. Since this thesis focuses on high-level decision-making, only discrete actions are considered.
    \item The state transition model $T(s' \mid s,a)$ describes the probability $\Pr(s' \mid s,a)$ that the system transitions to the next state $s' \in \mathcal{S}$ from state $s \in \mathcal{S}$ when action $a \in \mathcal{A}$ is taken.
    \item The reward function $R(s,a)$ returns a scalar reward or penalty $r$ for each action $a$ an agent takes in a given state $s$. The design of the reward function should reflect on the overall objective that the agent should maximize.
    \item The discount factor $\gamma \in [0,1)$ is a scalar that discounts the value of future rewards. A low discount factor $\gamma = 0$ would lead to an agent only consider the immediate reward.
\end{itemize}

A policy $\pi$ is defined as the mapping from state to action and the goal of the agent is to take a sequence of actions that maximize the accumulated reward. The value of being in a state while following a policy is described by the value function
\begin{align}
    V^\pi(s) = \mathbb{E} \left[ \sum_{k=0}^\infty \gamma^k R(s_t, a_t) | s_0 = s, \pi \right].
\end{align}
The optimal value function $V^*$ is unique and follows the Bellman equation: 
\begin{equation}
    V^*(s)= \max_{a} \left[ R(s, a) + \gamma \sum_{s'} T(s,a,s') V^*(s') \right].
    \label{eq:bellman}
\end{equation}

From the bellman equation one can deduce a state-action value function $Q(s,a)$ that satisfies $V^*(s)=\max_a Q(s,a)$. Given this Q function, a policy can be derived as $\pi(s) = \argmax_a Q(s,a)$. 

The discount factor $\gamma$ will affect the results of the optimization problem. A discount factor set close to $0$ will make immediate rewards more important while a $\gamma$ closer to $1$ would give some weight to expected future reward as well. 

\subsection{Partially observable Markov decision process}\label{section:pomdp}
Sometimes the agent does not have direct access to the entire state of the environment. In these cases, it is more common to use a \gls{pomdp}, which is an extension to the \gls{mdp} with the ability to handle state uncertainty. A \gls{pomdp} is defined by the tuple $(\mathcal{S},\mathcal{A},\mathcal{O},T,O,R,\gamma)$, where the state space, action space, transition model, reward and discount factor is the same as the \gls{mdp}, but it has two additional elements: 
\begin{itemize}
    \item The observation space $\mathcal{O}$, which represents all possible observations that the agent can receive. This can be both discrete and continuous.
    \item The observation model $O(o|s',a)$, which describes the probability of observing $o \in \mathcal{O}$ in a given state $s'$ after taking an action $a$: $O(o,s',a)=\Pr(o|s',a)$.
\end{itemize}

\todo{continue}
For a \gls{pomdp}, the agent takes an action $a$ from a given state $s$ and the environment transitions to the next state $s'$ according to the transition model $T$. The agent then receives an observation $o$ related to $s'$ and $a$ according to the observation model $O$. 

Because the state is no longer observable, the agent must reason about the history of taken actions and observations. Often, this history can be summarized in a statistic refered to as a belief, or belief state. A belief is a probability distribution over states so that $b: \mathcal{S} \rightarrow [0,1]$ and $\sum_{s} b(s)=1$, or $\int_{s} b(s)=1$ for continuous states. 

% \begin{itemize}
%     \item \todo{rewrite}
%     \item The observation space $\mathcal{O}$, which is the set of possible observations.
%     \item The observation model $O(o|s',a)$, which describes the probability of observing $o \in \mathcal{O}$ in state $s'$ after action $a$ has been taken.
% \end{itemize}

% More detail on how we model the \gls{pomdp} is described in section~\ref{sec:model}

\section{Reinforcement learning}
\todo{rewrite}
In many problems, the state transition probabilities or the reward function are not known. These problems can be solved by reinforcement learning techniques, in which the agent learns how to behave from interacting with the environment~\cite[Ch. 5]{Kochenderfer2015}. Compared to supervised learning, reinforcement learning presents some additional challenges. Since the data that are available to an RL-agent depends on its current policy, the agent must balance exploring the environment and exploiting the knowledge it has already gained. Furthermore, a reward that the agent receives may depend on a crucial decision that was taken earlier in time, which makes it important to assign rewards to the correct decisions.

RL algorithms can be divided into model-based and model-free approaches~\cite[Ch. 5]{Kochenderfer2015}. In the model-based versions, the agent first tries to estimate a representation of the state transition function $T$ and then use a planning algorithm to find a policy. On the contrary, as the name suggests, model-free RL algorithms do not explicitly construct a model of the environment to decide which actions to take. 
The model-free approaches can be further divided into value-based and policy-based techniques. Value-based algorithms, such as $Q$-learning, aim to learn the value of each state and thereby implicitly define a policy. Policy-based techniques instead search for the optimal policy directly in the policy space, either by policy gradient methods or gradient-free methods, such as evolutionary optimization. There are also hybrid techniques that are both policy and value-based, such as actor critic methods.

RL algorithms generally assume that the environment is modeled as an \gls{mdp}, i.e., that the state of the environment is known by the agent. However, in many cases of interest, only partial information about the state of the environment is available, which is modeled in the \gls{pomdp} framework. %For such cases, it is common to approximate the state by either the observation or a finite history of observations~\cite[Ch. 17]{Sutton2018}. The latter is referred to as a $k$-Markov approximation, where $k$ defines the length of the included history. For a sufficiently long history, the Markov property is assumed to approximately hold, even though the environment is partially observable.

\subsection{Deep Q-learning}
\subsection{QMDP}
\todo{Value itteration?}

% \tommy{CJ: Markov decision processes}

% %MDP
% Sequential decision-making problems in stochastic environments are commonly modeled as MDPs. Importantly, an MDP satisfies the Markov property, which requires that the probability distribution of the next state only depends on the current state and the action taken by the agent, i.e., not on the history of previous states or actions. An MDP is formally defined as the tuple $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$, which is described in the following list~\cite[Ch. 4]{Kochenderfer2015}:
% %
% \begin{itemize}
%     \item The state space $\mathcal{S}$ represents the set of all possible states of the environment. This set could consist of both discrete and continuous states.
%     \item The action space $\mathcal{A}$ represents the set of all valid actions the agent can take. This set could also consist of both discrete and continuous actions. However, since this thesis focuses on high-level decision-making, only discrete actions are considered here.
%     \item The state transition model $T(s'|s,a)$ describes the probability that the system transitions to state $s' \in \mathcal{S}$ from state $s \in \mathcal{S}$ when action $a \in \mathcal{A}$ is taken.
%     \item The reward function $R(s,a)$ returns a scalar reward $r$ for each state-action pair.
%     \item The discount factor $\gamma \in [0,1)$ is a scalar that discounts the value of future rewards. For a finite horizon MDP, $\gamma$ could also take the value~$1$.
% \end{itemize}

% A policy $\pi$ is a mapping from a state to an action, which could either be deterministic $a=\pi(s)$ or probabilistic $a \sim \pi(a|s)$. The value of being in a state while following a policy is described by the value function
% %
% \begin{align}
%     V^\pi(s) = \mathbb{E} \left[ \sum_{k=0}^\infty \gamma^k R(s_k, a_k) | s_0 = s, \pi \right].
% \end{align}
% %
% The goal of the agent is to find a policy which maximizes the value of each state.

% % In this framework, an agent chooses an action $a$, based on the current state $s$, then receives a reward $r$, and transitions to a new state $s'$. An MDP satisfies the Markov property, which assumes that the probability distribution of the next state only depends on the current state and action, and not on the history of previous states. The MDP is defined as the tuple $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $T$ is a state transition model, $R$ is a reward model, and $\gamma \in [0,1]$ is a discount factor. 
% % The state transition model $T(s' \mid s,a)$ describes the probability that the system transitions to state $s'$ from state $s$ when action $a$ is taken, and the reward model defines the reward of each step as $r=R(s,a,s')$.
% % The goal of an agent is to find a policy $\pi(s)$ which for every time step $t$ chooses an action that maximizes the future discounted return $R_t$, defined as
% % %
% % \begin{align}
% %     \label{eq:discountedReturn}
% %     R_t = \sum_{k=0}^\infty \gamma^k r_{t+k},
% % \end{align}
% % %
% % where $r_{t+k}$ is the reward at step $t+k$.


% %POMDP
% In many decision-making problems, the agent does not have direct access to the state of the environment. Such a problem is commonly modeled as a partially observable Markov decision process, which is an extension to the MDP framework that also models state uncertainty. A POMDP is formally defined as the tuple  $( \mathcal{S}, \mathcal{A}, \mathcal{O}, T, O, R, \gamma )$, where the state space, action space, transition model, reward function, and discount factor are defined as for an MDP. A POMDP has two additional components~\cite[Ch. 6]{Kochenderfer2015}:
% %
% \begin{itemize}
%     \item The observation space $\mathcal{O}$, which is the set of possible observations.
%     \item The observation model $O(o|s',a)$, which describes the probability of observing $o \in \mathcal{O}$ in state $s'$ after action $a$ has been taken.
% \end{itemize}
% %
% Since the agent does not have direct access to the current state in a POMDP, the agent needs to reason about the history of observations and actions. This history is often merged in a belief state $b$, which represents a probability distribution over the state space. In this case, the policy is a mapping from beliefs to actions $\pi(b)$.

% %In many decision-making problems, the exact state is not known by the agent and it only perceives observations $o$. A problem with state uncertainty can be modeled as a partially observable Markov decision process, which is defined by the tuple $( \mathcal{S}, \mathcal{A}, \mathcal{O}, T, O, R, \gamma )$. Compared to an MDP, the POMDP includes an additional observation space $\mathcal{O}$, and an observation model $O(o \mid s,a,s')$, which describes the probability of observing $o$ in state $s'$, after taking action $a$ in state $s$.


% For many real-world problems, it is not possible to represent the probability distributions $T$ or $O$ explicitly. For some solution approaches, only samples are needed, and then it is sufficient to define a generative model $G$ that samples a new state or observation from a given state and action, i.e., $s' \sim G(s,a)$ for the MDP case~\cite[Ch. 4]{Kochenderfer2015} and $(s', o) \sim G(s,a)$ for the POMDP case~\cite[Ch. 6]{Kochenderfer2015}.


% \tommy{Reinforcement learning}


% If all the elements $( \mathcal{S}, \mathcal{A}, T, R, \gamma )$ of an MDP are known, an agent can use this model to directly compute an optimal policy. Such a problem is often considered a planning problem. For small MDPs, dynamic programming\footnote{Dynamic programming refers to simplifying a complex problem by breaking it down into smaller sub-problems, often in a recursive manner.} techniques can provide an exact solution, which is calculated offline, i.e., before the agent is deployed in the environment. For example, in value iteration~\cite[Ch. 4]{Kochenderfer2015}, the Bellman operator is iteratively applied to the value function for all states,
% %
% \begin{align}
%     V_{n+1}(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'}T(s'|,a,s)V_n(s') \right].
% \end{align}
% %
% As $n$ goes to infinity, $V_n$ converges to the unique optimal value function $V^*$, and an optimal policy (not necessarily unique) is extracted by
% %
% \begin{align}
%     \pi(s) = \argmax_a \left[ R(s,a) + \gamma \sum_{s'}T(s'|,a,s)V^*(s') \right].
% \end{align}
% %
% However, for many real-world problems with high dimensional state spaces, it is intractable to compute and store a policy offline. Contrarily to offline methods, online search methods perform planning from the current state up to some horizon, when the agent has been deployed. Thereby, the agent can limit the computation to states that are reachable from the current state, which is often significantly smaller than the full state space~\cite[Ch. 4]{Kochenderfer2015}.

% In many problems, the state transition probabilities or the reward function are not known. These problems can be solved by reinforcement learning techniques, in which the agent learns how to behave from interacting with the environment~\cite[Ch. 5]{Kochenderfer2015}. Compared to supervised learning, reinforcement learning presents some additional challenges. Since the data that are available to an RL-agent depends on its current policy, the agent must balance exploring the environment and exploiting the knowledge it has already gained. Furthermore, a reward that the agent receives may depend on a crucial decision that was taken earlier in time, which makes it important to assign rewards to the correct decisions.

% RL algorithms can be divided into model-based and model-free approaches~\cite[Ch. 5]{Kochenderfer2015}. In the model-based versions, the agent first tries to estimate a representation of the state transition function $T$ and then use a planning algorithm to find a policy. On the contrary, as the name suggests, model-free RL algorithms do not explicitly construct a model of the environment to decide which actions to take. 
% The model-free approaches can be further divided into value-based and policy-based techniques. Value-based algorithms, such as $Q$-learning, aim to learn the value of each state and thereby implicitly define a policy. Policy-based techniques instead search for the optimal policy directly in the policy space, either by policy gradient methods or gradient-free methods, such as evolutionary optimization. There are also hybrid techniques that are both policy and value-based, such as actor critic methods.

% RL algorithms generally assume that the environment is modeled as an MDP, i.e., that the state of the environment is known by the agent. However, in many cases of interest, only partial information about the state of the environment is available, which is modeled in the POMDP framework. For such cases, it is common to approximate the state by either the observation or a finite history of observations~\cite[Ch. 17]{Sutton2018}. The latter is referred to as a $k$-Markov approximation, where $k$ defines the length of the included history. For a sufficiently long history, the Markov property is assumed to approximately hold, even though the environment is partially observable.

% Possibly add something about neural networks as function approximators somewhere in this section?
%    - No
