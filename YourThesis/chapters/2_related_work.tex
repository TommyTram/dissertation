% !TEX root=../../Thesis.tex
\chapter{Related work}\label{ch:related_work}
In recent years, decision-making for \gls{av}s in structured scenarios like intersections has attracted a lot of attention in the literature. This chapter provides a broad introduction to the primary research directions and outlines how the contributions in this thesis relate to existing work. However, it does not aim to provide a comprehensive survey of every approach.

% \todo{relate to the work in this thesis and why we choose DQN}
\section{Rule-based methods}
A skilled engineer can sometimes solve the decision-making problem for structured traffic scenarios using rule-based methods. One example of a rule-based method was implemented using hierarchical state machines to switch between predefined behaviors depending on what scenarios was encountered~\cite{Fletcher2008, darpa2008}. These methods were successful for a limited and controlled environment such as the Urban Challenge event, but it is difficult for an engineer to anticipate every situation that may occur in the real world and design a suitable strategy that can solve all of them, in particular when drivers are not following the law~\cite{Althoff2021}. The limitations of rule-based approaches motivate the choice of more adaptive and flexible methods, such as a planning-based or learning-based method. 
% One approach is to use the \gls{idm}~\cite{idm2000} to infer driver intent in urban intersections~\cite{Liebner2012} and another used a particle filter to estimate the parameters of the \gls{idm}~\cite{Hoermann2017}, both works showed promising results when evaluated on real-world data. Consequently, the \gls{idm} is used in this thesis to model the driving behavior of other vehicles.
% These accidents were mainly caused by driver inattention or reckless driving. 
% Hence, decision-making for \gls{av}s cannot only rely on traffic rules to be safe, but they also need to be prepared when other traffic participants do not follow them. Furthermore, rule-based approaches has difficulty generalizing to unknown situations and deal with uncertainties, such as the uncertainty of whether other traffic participants intend to follow the rules or not.


\section{Planning-based methods}
Planning-based methods treats the decision-making task as a motion planning problem. Commonly, a prediction model is used to predict the motion of the other agents, and then the behavior of the ego vehicle that is being controlled is planned accordingly. \Citet{Liebner2012} used to the \gls{idm} infer driver intent in urban intersections and \citet{Hoermann2017} used a particle filter to estimate the parameters of the \gls{idm}, both works showed promising results when evaluated on real-world data. Consequently, the \gls{idm} is used in this thesis to model the driving behavior of other vehicles.

One planning-based method is using \gls{mcts}~\cite{Browne2012}, but since the predictions are independent of the ego vehicle plan results in a reactive behavior~\cite{Hubmann2017, Sunberg2017}. Therefore, the interaction between the ego vehicle and other agents is not explicitly considered, but may happen implicitly by frequent replanning. \Gls{mcts} also requires extensive online computation and can be hard to scale in complex traffic situations with an increasing number of traffic participants. 

Another approach to solve the motion planning problem is to use optimal control, which was applied to highway driving scenarios by \citet{Werling2010}. Since human behavior is complex and varies between individuals, a study by \citet{Damerow2015} use a probabilistic prediction as input to the motion planning, which aims to minimize the risk of collision during an intersection scenario. While \citet{batkovic2019} used a robust scenario \gls{mpc} approach to handle uncertain multi-modal road users. Other approaches to motion planning for autonomous driving are provided in the surveys by \citet{Gonzales2016,Paden2016}. However, these planning-based methods rely on the accuracy of the prediction models and require a lot of on-board computing power which may be limited in an \gls{av}.

% It is common to model decision-making problems as Markov decision processes or partially observable Markov decision processes (POMDPs)~\cite{Kochenderfer2015}. This mathematical framework allows modeling of uncertainty in the current state, uncertainty in the future development of the traffic scene, and modeling of an interactive behavior. The task of finding the optimal policy for a POMDP is most often intractable, but many approximate methods exist. One way to group these methods is in offline and online methods. There are powerful offline algorithms for planning in POMDPs, which can solve complex situations. One example is shown by Brechtel et al., which proposes a solution to how measurement uncertainty and occlusions in an intersection can be handled~\cite{Brechtel2014}.

\section{Learning based methods}
Learning-based approaches offer the ability to learn from experience, adapt to new situations, and make decisions based on a wide range of scenarios, rather than relying on predefined rules or models. \Gls{rl} methods can help relieve the burden of designing hand-crafted solutions for all possible scenarios~\cite{Sutton2018, Isele2018}. The work by \citet{Mnih2013} showed a \gls{dqn} that achieved impressive results in training agents to play Atari games from raw pixel inputs using experience replay, highlighting \gls{dqn}s ability to learn complex behaviors from high-dimensional sensory data, a key requirement for autonomous vehicles navigating intersections.  
In order to handle the uncertainty of predicting other traffic participants' behaviors or intentions, the literature formulates the problem of driving under uncertainty as a \gls{pomdp}~\cite{Kochenderfer2015}. \Gls{drqn} approaches, such as the ones from \citet{HausknechtS15drqn, zhu2018improving}, showed some promise solving \gls{pomdp} with non-observable states by leveraging past observations or actions. %\gls{mcts} was even combined with \gls{rl} to move the extensive computation from online to offline~\cite{Hoel2018}.

Another approach by ~\citet{Bouton2017} used belief states to capture uncertainties in the environment. The belief state can be used to model the probability distribution over the uncertain world states, e.g., the intention of other road users. \Citet{wang2023} decoupled the belief state modeling (via unsupervised learning) from policy optimization (via \gls{rl}) and \citet{Littman1995} claimed that having full observability at learning time, combined with knowing what will not be observable at deployment time, enables an \gls{rl} agent to learn a policy that is more robust to its unobservable states. 

Advantage of \gls{dqn} methods, compared to planning based methods, is that \gls{dqn} learns through interaction with the environment. Experience replay allows \gls{dqn} to learn efficiently from past experiences, improving its performance with less real-world data compared to methods without it. This capability enables autonomous vehicles to adapt to unseen situations and make informed decisions based on accumulated experiences at intersections. While \gls{dqn} doesn't directly address driver intentions, it can learn to infer them indirectly from traffic patterns and historical data. This allows for adaptive decision-making based on the perceived likelihood of driver actions at intersections. Therefore, \gls{dqn} is a suitable choice for this thesis due to its adaptability and efficiency in learning from complex, dynamic environments.
