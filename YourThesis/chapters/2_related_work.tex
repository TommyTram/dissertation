% !TEX root=../../Thesis.tex
\chapter{Related work}\label{ch:related_work}

In recent years, decision-making for \gls{av}s in structured scenarios like intersections has attracted a lot of attention in the literature. This chapter gives a broad introduction to the main research directions, but it does not aspire to provide a comprehensive survey of each class of approaches.

\section{Rule-based methods}
A skilled engineer can sometimes solve the decision-making problem for structured traffic scenarios using rule-based methods. One example of a rule-based method was implemented using hierarchical state machines to switch between predefined behaviors depending on what scenarios was encountered~\cite{Fletcher2008, darpa2008}. These methods were successful for a limited and controlled environment such as the Urban Challenge event, but it is difficult for an engineer to anticipate every situation that may occur in the real world and design a suitable strategy that can solve all of them, in particular when drivers are not following the law~\cite{Althoff2021}. 
One approach is to use the \gls{idm}~\cite{idm2000} to infer driver intent in urban intersections~\cite{Liebner2012} and another used a particle filter to estimate the parameters of the \gls{idm}~\cite{Hoermann2017}, both works showed promising results when evaluated on real-world data. 
% These accidents were mainly caused by driver inattention or reckless driving. 
% Hence, decision-making for \gls{av}s cannot only rely on traffic rules to be safe, but they also need to be prepared when other traffic participants do not follow them. Furthermore, rule-based approaches has difficulty generalizing to unknown situations and deal with uncertainties, such as the uncertainty of whether other traffic participants intend to follow the rules or not.


\section{Planning methods}
% \todo{Motion planning, Predicting motion of surrounding vehicles, reactive (not intersactive)}
Another group of algorithms treats the decision-making task as a motion planning problem. Commonly, a prediction model is used to predict the motion of the other agents, and then the behavior of the ego vehicle that is being controlled is planned accordingly. One planning method is using \gls{mcts}~\cite{Browne2012}, but since the predictions are independent of the ego vehicle plan results in a reactive behavior~\cite{Hubmann2017, Sunberg2017}. Therefore, the interaction between the ego vehicle and other agents is not explicitly considered, but may happen implicitly by frequent replanning. %\Gls{mcts} also requires extensive online computation and can be hard to scale in complex traffic situations with an increasing number of traffic participants.

A third approach to solve the motion planning problem is to use optimization-based techniques, for example optimal control, which was applied to highway driving scenarios by Werling et al.~\cite{Werling2010}. Since human behavior is complex and varies between individuals, some algorithms use a probabilistic prediction as input to the motion planning. This is for example shown in a study by Damerow et al.~\cite{Damerow2015}, which aims to minimize the risk during an intersection scenario. Other approaches to motion planning for autonomous driving are provided in the surveys by Gonzáles et al.~\cite{Gonzales2016} and Paden et al.~\cite{Paden2016}. 

% It is common to model decision-making problems as Markov decision processes or partially observable Markov decision processes (POMDPs)~\cite{Kochenderfer2015}. This mathematical framework allows modeling of uncertainty in the current state, uncertainty in the future development of the traffic scene, and modeling of an interactive behavior. The task of finding the optimal policy for a POMDP is most often intractable, but many approximate methods exist. One way to group these methods is in offline and online methods. There are powerful offline algorithms for planning in POMDPs, which can solve complex situations. One example is shown by Brechtel et al., which proposes a solution to how measurement uncertainty and occlusions in an intersection can be handled~\cite{Brechtel2014}.

% MPC cite ivo:
% A robust scenario MPC approach for uncertain multi-modal obstacles,
% Real-Time Constrained Trajectory Planning and Vehicle Control for Proactive Autonomous Driving With Road Users.


\section{Learning based methods}
% \todo{POMDP model, online and offline solvers}
In order to handle the uncertainty of predicting other traffic participants' behaviors or intentions, the literature formulates the problem of driving under uncertainty as a \gls{pomdp}~\cite{Kochenderfer2015}. 
Learning-based approaches, like \gls{rl}~\cite{Sutton2018, Isele2018}, can help relieve the burden of designing hand-crafted solutions for all possible scenarios and \Gls{drqn} approaches, \cite{HausknechtS15drqn, zhu2018improving}, showed some promise solving \gls{pomdp} with non-observable states by leveraging past observations or actions. %\gls{mcts} was even combined with \gls{rl} to move the extensive computation from online to offline~\cite{Hoel2018}.

Another approach to estimating the intention is to introduce belief states to capture uncertainties in the environment~\cite{Bouton2017}. The belief state can be used to model the probability distribution over the uncertain world states, e.g., the intention of other road users. One paper~\cite{wang2023} decoupled the belief state modeling (via unsupervised learning) from policy optimization (via \gls{rl}) and claimed that having full observability at learning time, combined with knowing what will not be observable at deployment time, enables an \gls{rl} agent to learn a policy that is more robust to its unobservable states. 
One advantage of \gls{rl} methods, compared to planning based methods, is that a model of the environment is not required, i.e., the transition probabilities between different states are not assumed to be known. Many \gls{rl} methods provide a general framework and an agent could, in theory, learn how to behave correctly in all possible driving situations.
% Another approach using full observability at learning time is QMDP~\cite{Littman1995}, where a model is first trained on the full state space, including the unobservable states, and later evaluated the model on the belief state. Training on full observability can only be done when the unobservable state can be obtained, e.g., using ground truth data and labeling.


% \tommy from belief 

% In order to handle the uncertainty of predicting other traffic participants' behaviors or intentions, the literature formulates the problem of driving under uncertainty as a \gls{pomdp}, e.g., with intentions as a non-observable state. \gls{pomdp}s can be solved by simulating all possible future motions given different potential behaviors and/or intentions~\cite{Hubmann2017, Sunberg2017}, e.g. \gls{mcts}. Unfortunately, \gls{mcts} requires extensive online computation and can be hard to scale in complex traffic situations with an increasing number of traffic participants, especially when a participant's actions are interdependent on all other participants’ actions. 
% Learning-based approaches, like \gls{rl}~\cite{Sutton2018, Isele2018}, can help relieve the burden of designing hand-crafted solutions for all possible scenarios and \Gls{drqn} approaches, \cite{HausknechtS15drqn, zhu2018improving}, showed some promise solving \gls{pomdp} with non-observable states by leveraging past observations or actions. %\gls{mcts} was even combined with \gls{rl} to move the extensive computation from online to offline~\cite{Hoel2018}.
% One such approach was proposed in our previous work~\cite{Tram2018}, where a decision-making policy that identifies and chooses a gap in-between cars in an intersection scenario by using a deep Q-learning~\cite{Mnih2013} method and an \gls{lstm} network architecture~\cite{lstm1997}.
% The results showed that the policy solved the decision-making problem up to $97\%$ of the time under perfect sensing conditions. The few cases where the \gls{av} ended up in collisions occurred when the \gls{av} and another vehicle had the same velocity and the same distance to the intersection, i.e., the policy had difficulties sorting out the situation because the estimation of the hidden state (intention) was embedded in the neural network architecture. The prediction of surrounding vehicles was especially difficult for low velocities. 

% To explicitly estimate the intention of other drivers is not an easy task~\cite{Amsalu2017}. 
% \citet{Liebner2012} proposed to estimate driver behavior using a driver model, the so-called \gls{idm} \cite{idm2000}, to infer driver intent in urban intersections, and \citet{Hoermann2017} used a particle filter to estimate the parameters of the \gls{idm}, both works showed promising results when evaluated on real-world data. 
% Another approach to estimating the intention is to introduce belief states to capture uncertainties in the environment~\cite{Bouton2017}. The belief state can be used to model the probability distribution over the uncertain world states, e.g., the intention of other road users. \Citet{wang2023} decoupled the belief state modeling (via unsupervised learning) from policy optimization (via \gls{rl}) and claimed that having full observability at learning time, combined with knowing what will not be observable at deployment time, enables an \gls{rl} agent to learn a policy that is more robust to its unobservable states. Another approach using full observability at learning time is QMDP~\cite{Littman1995}, where a model is first trained on the full state space, including the unobservable states, and later evaluated the model on the belief state. Training on full observability can only be done when the unobservable state can be obtained, e.g., using ground truth data and labeling.




% \tommy{rewrite text}
% %Rule based methods
% Early approaches to tactical decision-making for autonomous vehicles often used rule-based methods, commonly implemented as handcrafted state machines. For example, during the DARPA Urban Challenge, a rule-based approach was adopted by the winning team from the Carnegie Mellon University, where different modules handled the behavior for the different driving scenarios that were encountered~\cite{darpaCMU}. 
% % Other participants, such as Stanford and Virginia Tech, used similar strategies~\cite{Bacha2008, darpaStanford}. 
% While successful for a limited and controlled environment such as the Urban Challenge event, it is unlikely that rule-based approaches could scale to handle the complexity and diversity of real-world driving.

% %Motion planning, predicting motion of surrounding vehicles, reactive (not interactive)
% Another group of algorithms treats the decision-making task as a motion planning problem. Commonly, a prediction model is used to predict the motion of the other agents, and then the behavior of the vehicle that is being controlled, henceforth referred to as the ego vehicle, is planned accordingly. This results in a reactive behavior, since the predictions are independent of the ego vehicle plan. Therefore, interaction between the ego vehicle and other agents is not explicitly considered, but may happen implicitly by frequent replanning.
% Search-based planners typically discretize the state space and then apply Dijkstra's algorithm~\cite{Dijkstra1959} or one of the algorithms from the A* family~\cite{Hart1968}. These techniques were also commonly used during the DARPA Urban Challenge~\cite{Bacha2008, darpaStanford}. However, since real-time performance can be hard to achieve with graph-search algorithms, sampling-based algorithms such as rapidly-exploring random trees~\cite{Lavalle1998} have been used for motion planning, e.g., by Karaman et al.~\cite{Karaman2011}. 
% A third approach to solve the motion planning problem is to use optimization-based techniques, for example optimal control, which was applied to highway driving scenarios by Werling et al.~\cite{Werling2010}. Since human behavior is complex and varies between individuals, some algorithms use a probabilistic prediction as input to the motion planning. This is for example shown in a study by Damerow et al.~\cite{Damerow2015}, which aims to minimize the risk during an intersection scenario.
% Additional approaches to motion planning for autonomous driving are provided in the surveys by Gonzáles et al.~\cite{Gonzales2016} and Paden et al.~\cite{Paden2016}. 


% %POMDP model, online and offline solvers
% It is common to model decision-making problems as Markov decision processes or partially observable Markov decision processes (POMDPs)~\cite{Kochenderfer2015}. This mathematical framework allows modeling of uncertainty in the current state, uncertainty in the future development of the traffic scene, and modeling of an interactive behavior. The task of finding the optimal policy for a POMDP is most often intractable, but many approximate methods exist. One way to group these methods is in offline and online methods. There are powerful offline algorithms for planning in POMDPs, which can solve complex situations. One example is shown by Brechtel et al., which proposes a solution to how measurement uncertainty and occlusions in an intersection can be handled~\cite{Brechtel2014}. In their work, an offline planner precomputes the policy by using a state representation that is learned for the specific scenario. A similar approach is adopted by Bai et al.\ for an intersection scenario~\cite{Bai2014}. The main drawback of these offline methods is that they are designed for specific scenarios. Due to the large number of possible real-world scenarios, it is challenging to precalculate a policy that is generally valid.


% %Online solvers
% Online methods compute a policy during execution, which makes them more versatile than offline methods. However, the limited available computational resources require a careful problem formulation and limit the solution quality.
% % Ulbrich et al.\ use a POMDP framework to make decisions on when to change lanes during highway driving~\cite{Ulbrich2015}. In order to make it possible to solve the problem with an exhaustive search, a problem-specific high-level state space is created, which consists of states that represent whether or not a lane change is possible or beneficial. However, due to the specialized state space, it is hard to generalize this method.
% Another online approach for solving a POMDP is the family of Monte Carlo tree search algorithms~\cite{Browne2012}, which is used by Sunberg et al.\ to make lane changing decisions on a highway~\cite{Sunberg2017}. 
% % In order to handle the continuous state description, the tree search is extended with a technique called progressive widening~\cite{Couetoux2011}. 
% Furthermore, other drivers' intentions are estimated with a particle filter. %\cite{Lenz2016} also applied a MCTS method for lane change decisions.
% A hybrid approach between offline and online planning is pursued in a study by Sonu et al., where a hierarchical decision-making structure is used~\cite{Sonu2018}. The decision-making problem is modeled on two levels as MDPs, since full observability is assumed. The high-level MDP is solved offline by value iteration and the low-level MDP is solved online with MCTS.
% % Requires transition model, hard to obtain.


% \vspace{8pt}

% \tommy{Learning-based methods}
% \tommy{rewrite}

% In planning-based methods, different algorithms are used to find a policy that maximizes the utility of the agent's behavior by using a model of the system. However, data-driven approaches are fundamentally different, since with this class of methods, an agent instead learns how to behave from observing data. This section describes different types of data-driven approaches that have been explored for autonomous driving.

% % Imitation learning, behavioral cloning
% An intuitive approach is to collect data from when an expert is performing a task and then use supervised learning to imitate the behavior of the expert. This method is often referred to as behavioral cloning and was first applied to autonomous driving in the ALVINN project~\cite{Pomerleau1989}. Unfortunately, for many cases, behavioral cloning suffers from compounding errors, which refers to the problem when small mistakes gradually push the agent further away from the training distribution, into states from which the agent does not know how to recover. This problem can be mitigated by an active learning approach, where an expert can be queried during the training process~\cite{Ross2011}, which for example has been applied to autonomous driving by Kelly et al.~\cite{Kelly2019}. An alternative is to synthesize data by perturbing the expert's driving, which was done in a study by Bansal et al.~\cite{Bansal2019}. Generative adversarial imitation learning~\cite{Ho2016} provides another method to handle the compounding error problem and has showed promising results in different highway driving scenarios~\cite{Kuefler2017}.

% %RL
% Reinforcement learning is conceptually different from supervised learning, since labeled input-output samples are not available. Instead, the agent learns how to make decisions from interacting with the environment.
% %Reinforcement learning is conceptually different from supervised learning, which is used for behavioral cloning, since labeled input/output samples are not available in RL problems. Instead, an RL-based agent typically learns how to make decisions from interacting with an environment, in which the agent occasionally receives a reward signal that tells the agent how well it is performing.
% RL methods are versatile, and have proven successful in various domains, such as playing Atari games~\cite{Mnih2015}, in continuous control~\cite{Lillicrap2015}, reaching a super human performance in the game of Go~\cite{Silver2017}, and beating the best chess computers~\cite{Silver2017chess}. One advantage of RL methods, compared to planning based methods, is that a model of the environment is not required, i.e., the transition probabilities between different states are not assumed to be known. Furthermore, many RL methods provide a general framework and an agent could, in theory, learn how to behave correctly in all possible driving situations.
% %
% During the last few years, many papers have addressed the task of applying RL approaches to autonomous driving. For example, DQN-based agents were used by Isele et al.~\cite{Isele2018} for navigating through different intersection scenarios, with varying driver intentions and occlusions. Commonly, a high-level action space is used together with the DQN algorithm. Other studies use policy gradient RL methods to directly control the speed and the steering angle of the vehicle, for example in lane changing and urban scenarios~\cite{Wang2019_ddpg, Chen2019}.
% Safety of the RL-based agents has been addressed by restricting dangerous actions, either by heuristics~\cite{Mukadam2017}, linear temporal logic~\cite{Bouton2019}, or using an underlying motion planner with hard constraints~\cite{Shalev2016}.

% A majority of these studies perform both the training and evaluation in simulated environments, whereas some train the agent in a simulator and then apply the trained agent in the real world~\cite{Pan2017, Bansal2019}, or for some limited scenarios, the training itself is also performed in the real world~\cite{Kendall2019}. 
% Overviews of RL-based studies for autonomous driving are given by Kiran et al.~\cite{Kiran2021} and by Ye et al.~\cite{Ye2021}.
% % Add reference to Pin Wang's new review paper. Possibly replace one of the other survey papers?
% %    - DONE

% Both planning-based and RL-based methods use a reward function to find a policy that maximizes the cumulative future reward. However, for complex tasks such as autonomous driving, the design of the reward function is in itself a complicated task. For limited scenarios, the reward function can be manually specified and tuned until the agent finds a desired behavior, which is referred to as reward shaping~\cite{Ng1999}. However, for realistic scenarios, the number of possible reward features is massive and how to balance rewards related to, e.g., safety and efficiency is a complex issue. A practical approach is to instead learn the reward function from the behavior of human drivers by inverse reinforcement learning (IRL)~\cite{Ng2000}. An IRL approach was for example used by Kuderer et al.\ to learn the individual preferences of human drivers with different driving styles~\cite{Kuderer2015}. Sharifzadeh et al.\ combined IRL with DQN and Wang et al.\ used an adversarial IRL approach to simultaneously obtain both the reward function and the policy for different lane-changing scenarios~\cite{Sharifzadeh2016, Wang2019}. 
% Zhu et al.~\cite{Zhu2021} provide an overview of IRL applied to autonomous driving.
% Except for performing planning and RL, the learned reward function can also be used to predict the behavior of other traffic participants. IRL for predictions was for example used by Ziebart et al.\ for pedestrians~\cite{Ziebart2009}, by Sun et al.\ for human drivers in intersections~\cite{Sun2019}, and by Sadigh for highway driving situations~\cite{Sadigh2016}.

