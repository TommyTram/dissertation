\section{Introduction}\label{sec:intro}

%%% GENERAL OVERVIEW
Deploying autonomous agents in the real world poses a wide variety of challenges. As in~\citep{dulac2021challenges}, we are often required to learn the real-world model with limited data, and use it to plan so as to achieve satisfactory performance in the real world. There might also be safety and reproducibility constraints, which require us to track a model of the real-world environment~\citep{skirzynski2021automatic}.
In light of these challenges, we attempt to construct a framework that can aptly deal with optimal decision making for a novel task, by leveraging knowledge external to the task. As the novel task is unknown, we adopt the Reinforcement Learning (RL)~\citep{sutton2018reinforcement} framework to guide an agent's learning process and to achieve near-optimal decisions.
%of decision-making under uncertainty, %which has a deep history going back to~\citet{von2007theory}, to operations research~\citep{bell1982regret} to learning~\citep{bertsekas1997nonlinear}. One such approach that we adopt in this work, is the Reinforcement Learning (RL)~\citep{sutton2018reinforcement} framework to guide an agent's learning process and to achieve near1-optimal decisions. 

An RL agent interacts directly with the environment to improve its performance. Specifically, in model-based RL, the agent tries to learn a model of the environment and then uses it to improve performance~\citep{moerland2020model}. In many applications, the depreciation in performance due to sub-optimal model learning can be paramount. For example, if the agent is interacting with living things or expensive equipment, decision-making with an imprecise model might incur significant cost~\citep{polydoros2017survey}. In such instances, boosting the model learning by leveraging external knowledge from the existing models, such as simulators, physics-driven engines, etc., can be of great value~\citep{taylor2008transferring}. A model trained on simulated data may perform reasonably well when deployed in a new environment, given the novel environment is \emph{similar enough} to the simulated model. 
Also, RL algorithms running on different environments yield data and models that can be used to plan in another similar enough real-life environment.
In this work, we study the problem, where we have access to multiple source models built using simulators or data from other environments, and we want to transfer the source models to perform model-based RL in a different real-life environment.

% \begin{example}
Let us consider that a company is designing autonomous driving agents for different countries in the world. The company has designed two RL agents that have learned to drive in USA and UK. Now, the company wants to deploy a new RL agent in India. Though all the RL agents are concerned with the same task, i.e. driving, the models encompassing driver behaviours, traffic rules, signs etc., can be different for each of them. For example, UK and India have left-handed traffic, while the USA has right-handed traffic.  However, learning a new controller \emph{specifically} for every new geographic location is computationally expensive and also time-consuming, as both data collection and learning take time. Thus, the company might like to use the models learnt for UK and USA, to estimate the model for India, and use it further to build a new autonomous driving agent (RL agent). Hence, being able to transfer the source models to the target environment allows the company to use existing knowledge to build an efficient agent faster and resource efficiently.
%Consider the case where an autonomous driving agent has learned how to drive in the US but now is tasked with driving in the UK. Should you expect it to perform well in both cases? Disregarding the fact of right-hand vs. left-hand traffic, the traffic rules, signs and driver behavior may differ. However, learning a new controller \emph{specifically} for every new geographic location is intractable. Being able to use existing knowledge obtained from one agent to bootstrap learning of a new agent may very well be a good compromise in this setting.\dbcomment{edit it with one more source model}
% \end{example}
%%% CONTRIBUTION
\textit{We address this problem of model transfer from source models to a target environment in order to plan efficiently.} We observe that this problem falls at the juncture of \emph{transfer learning} and \emph{reinforcement learning}~\citep{taylor2009transfer,lazaric2012transfer,laroche2017transfer}. %allows us to bootstrap learning of new agents in novel environments. 
%Our work is positioned within the field of TRL, which has been investigated thoroughly. 
\citet{lazaric2012transfer} enlists three approaches to transfer knowledge from the \emph{source tasks} to a \emph{target task}. (i) \emph{Instance transfer:} data from the source tasks is used to guide decision-making in the novel task~\citep{taylor2008transferring}. (ii) \emph{Representation transfer:} a representation of the task, such as learned neural network features, are transferred to perform the new task~\citep{zhang2018decoupling}. (iii) \emph{Parameter transfer:} the parameters of the RL algorithm or \emph{policy} are transferred~\citep{rusu2015policy}. In our paper, the source tasks are equivalent to the source models, and the target task is the target environment. Moreover, we adopt the \textbf{model transfer} approach (MTRL), which encompasses both (i) and (ii) (Section~\ref{sec:trl}). 

\citet{langley2006transfer} describes three possible benefits of transfer learning. The first is~\textbf{learning speed improvement}, i.e. decreasing the amount of data required to learn the solution. Secondly, \textbf{asymptotic improvement}, where the solution results in better asymptotic performance. Lastly, \textbf{jumpstart improvement}, where the initial proxy model serves as a better starting solution than that of one learning the true model from scratch. In this work, we propose a new algorithm to transfer RL that achieves both learning speed improvement and jumpstart improvement (Section~\ref{sec:experiments}). However, we might not find an asymptotic improvement in performance if compared with the best and unbiased algorithm in the true setting. Rather, we aim to achieve a model estimate that can allow us to plan almost optimally in the target MDP (Section~\ref{sec:bounds}). %Indeed, from our problem formulation the reason for this will be clear and for many settings not only will we attain near-optimal performance but actual optimal performance.

\textbf{Contributions.} In brief, we aim to answer the two questions:\\
1. \emph{How can we accurately construct a model using a set of source models for an RL agent deployed in the wild?} \\
2. \emph{Does the constructed model allows us to perform efficient planning and yield improvements over learning from scratch?}

In this paper, we address these questions as follows:

1. \textit{A Taxonomy of MTRL:} First, we concretely formulate the problem with Markov Decision Processes (MDPs) setting of RL. We further provide a taxonomy of the problem depending on a discrete or continuous set of source models, and whether the target model is realisable by the source models (Section~\ref{sec:trl}).

2. \textit{Algorithm Design with MLE:} Following that, we design a two-stage algorithm MLEMTRL to plan in an unknown target MDP (Section~\ref{sec:algorithms}). In the first-stage, MLEMTRL uses a Maximum Likelihood Estimation (MLE) approach to estimate the target MDP using the source MDPs. In the second stage, MLEMTRL uses the estimated model to perform model-based planning. We instantiate MLEMTRL for discrete state-action (tabular) MDPs and Linear Quadratic Regulators (LQRs). We also derive a generic bound on the goodness of the policy computed using MLEMTRL (Section~\ref{sec:bounds}).

3. \textit{Performance Analysis:} In Section~\ref{sec:experiments}, we empirically verify whether MLEMTRL improves the performance for unknown tabular MDPs and LQRs than learning from scratch. MLEMTRL exhibits learning speed improvement and asymptotic improvement for tabular MDPs. In case the of LQRs, it incurs learning speed improvement and jumpstart improvement. We also observe that improvements obtained by using MLEMTRL depend on the similarity between the target and source models. The more similar the target and the source models better is the performance of MLEMTRL, as indicated by the theoretical analysis.

% This is accomplished through the transfer reinforcement learning framework, by efficiently making use of knowledge from source tasks when interacting with the target task. The proposed algorithm, MLEMTRL, displays superiority compared to an agent learning the novel task from scratch for two of the main objectives listed by~\citep{taylor2009transfer}, namely, jumpstart improvement and learning speed improvement. We demonstrate the performance of the algorithm in a tabular and LQR setting and prove worst-case regret bounds for the algorithm in both the realisable and non-realisable settings. Finally, we show a connection between model dissimilarity and degradation in performance, indicating transfer reinforcement learning makes the most sense when the tasks are similar.

% \textbf{Outline.}
% In this work, we begin by introducing the necessary background knowledge of MDPs and the appropriate likelihood functions for said MDP classes, in Section~\ref{sec:background}. Next, we instantiate the \emph{transfer reinforcement learning} problem in Section~\ref{sec:trl} and the various sub-settings of the problem. Further, we describe how to achieve optimality (or near-optimality) in this setting. Later, in Section~\ref{sec:algorithms}, we introduce the proposed algorithm, MLEMTRL, and the two-stage process it contains. Namely, \emph{model identification} and \emph{model-based planning}. In Section~\ref{sec:bounds} we provide theoretical worst-case bounds for the proposed algorithm and empirical results in Section~\ref{sec:experiments}. Finally, in Section~\ref{sec:discussion} we summarise our findings and describe future directions of this work.

Before elaborating on the contributions, we posit this work in the existing literature (Section~\ref{sec:related}) and discuss the necessary background knowledge of MDPs and MLEs (Section~\ref{sec:background}).
%%%
\section{Related Works}\label{sec:related}
Our work on Model Transfer Reinforcement Learning (MTRL) is situated in the field of Transfer RL (TRL), and also is closely related to the multi-task RL and Bayesian multi-task RL literature. In this section, we elaborate on these connections.

TRL is widely studied in Deep Reinforcement Learning (DRL). \citep{zhu2020transfer} introduces different ways of transferring knowledge, such as \emph{policy transfer}, where the set of source MDPs $\mathcal{M}_s$ has a set of expert policies associated with them. The expert policies are used together with a new policy for the novel task by transferring knowledge from each policy. \citep{rusu2015policy} uses this approach, where a student learner is combined with a set of teacher networks to guide learning in multi-task RL. \citep{parisotto2015actor} develops an actor-critic structure in order to learn how to transfer its knowledge to new domains.   \citep{arnekvist2019vpe} invokes generalisation across Q-functions by learning a master policy. Here,\textit{ we focus on model transfer instead of policy}.

Another seminal work in TRL, \citep{taylor2009transfer} distinguishes between \emph{multi-task learning} and \emph{transfer learning}. Multi-task learning deals with problems where the agent aims to learn from a distribution over scenarios, whereas transfer learning makes no specific assumptions about the source and target tasks. Thus, in transfer learning, the tasks could involve different state and action-spaces, and different transition dynamics. Specifically, we focus on \textbf{model-transfer} approach to TRL, where the state-action spaces and also dynamics can be different~\citep{atkeson1997comparison}. \citep{atkeson1997comparison} performs model-transfer for a target task with an identical transition model. Thus, the main consideration is to transfer knowledge to tasks with same dynamics but varying rewards. \citep{laroche2017transfer} assumes a context similar to that of~\citep{atkeson1997comparison}, where the model dynamics are identical across environments. In our work, we rather assume that the reward function is the same but the transition models are different. We believe this is an interesting questions as the harder part of learning an MDP is learning the transition model. 
These works explicate a deep connection between the fields of \emph{multi-task learning} and \emph{TRL}. In general, TRL can be viewed as an extension of multi-task RL, where multiple tasks can either be learned simultaneously or have been learned \textit{a priori}. This flexibility allows us to learn even in settings where the state-actions and transition dynamics are different among tasks. \citep{rommel2017aircraft} describes a multi-task Maximum Likelihood Estimation (MLE) procedure for optimal control of an aircraft. They identify a mixture of Gaussians, where the mixture is over each of the tasks. Here, \textit{we adopt an MLE approach to TRL in order to optimise performance for the target MDP (or a target task) than restricting to a mixture of Gaussians.}

The Bayesian approach to multi-task RL~\citep{wilson2007multi, lazaric2010bayesian} tackles the problem of learning jointly how to act in multiple environments. \citep{lazaric2010bayesian} handles the \emph{open-world assumption}, i.e. the number of tasks is unknown. This allows them to transfer knowledge from existing tasks to a novel task, using value function transfer. However, this is significantly different from our setting, as we are considering \emph{model-based transfer}. Further, \textit{we adopt an MLE-based framework in lieu of the full Bayesian procedure described in their work}.
In Bayesian RL, \citep{tamar2022regularization} also investigates a learning technique to generalise over multiple problem instances. By sampling a large number of instances, the method is expected to learn how to generalise from the existing tasks to a novel task. We do not assume access to such a prior or posterior distributions to sample from.

There is another related line of work, namely multi-agent transfer RL~\citep{da2019survey}. For example, \citep{liang2023federated} develops a TRL framework for autonomous driving using federated learning. They accomplish this by aggregating knowledge for independent agents. This setting is significantly different from the general transfer learning but could be incorporated if each of the source tasks were being learned simultaneously as the target task. This requires cooperation among agents and is out of the scope of this paper.

%\newpage
\section{Background}\label{sec:background}

In this section, we introduce the important concepts on which this work is based upon. Firstly, we introduce the way we model the dynamics of the tasks. Secondly, we describe the Maximum Likelihood Estimation (MLE) framework used in this work.

\subsection{Markov Decision Process}\label{sec:mdp}
We study sequential decision-making problems that can be represented as Markov Decision Processes (MDPs)~\citep{puterman2014markov}. An MDP $\mdp$ consists of a discrete or continuous state space denoted by $\mathcal{S}$, a discrete or continuous action-space $\mathcal{A}$, a reward function $\mathcal{R} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ which determines the quality of taking action $a$ in state $s$, and a transition function $\mathcal{T} : \mathcal{S} \times \mathcal{A} \rightarrow \Delta\mathcal{S}$ inducing a probability distribution over the successor states $s'$ given a current state $s$ and action $a$. Finally, in the infinite-horizon formulation, a discount factor $\gamma \in [0, 1)$ is assigned. The overarching objective for the agent is to compute a decision-making policy $\pol : \mathcal{S} \rightarrow \Delta(\mathcal{A})$ that maximises the expected sum of future discounted rewards up until the horizon $T$: $V_\mdp^{\pol}(s) = \mathbb{E}\Big[\sum_{t=0}^T \gamma^t \mathcal{R}(s_t, a_t)\Big]$. $V_\mdp^{\pol}(s)$ is called the value function of policy $\pol$ for MDP $\mdp$. The optimal value function is denoted as $\mdp$ is $V_\mdp^{*} = V_\mdp^{\pol^{*}}$. The technique used to obtain the the optimal policy $\pol^{*} = \underset{\pol}{\sup} \, V_\mdp^{\pol}$ depends on the MDP class. The MDPs with discrete state-action spaces are referred to as tabular MDPs. In this paper, we also study a specific class of MDPs with continuous state-action spaces, namely the Linear Quadratic Regulators (LQRs)~\citep{kalman1960new}. In tabular MDPs, we employ \textsc{ValueIteration}~\citep{puterman2014markov} for model-based planning whereas in the LQR setting we use \textsc{RiccatiIteration}~\citep{willems1971least}. 

The standard metric used to measure the performance of a policy $\pol$~\citep{bell1982regret} for an MDP $\mdp$ is \textit{regret} $R(\mdp, \pol)$. Regret is the difference between the optimal value function and the value function of $\pol$. In this work, we extend the definition of regret for MTRL, where the optimality is taken with respect to a policy class in the target MDP.


\subsection{Maximum Likelihood Estimation}
One of the most popular methods of constructing point estimators is the \emph{Maximum Likelihood Estimation} (MLE)~\citep{casella2021statistical} framework. Given a density function $f(x \, | \, \theta_1, \hdots, \theta_n)$ and associated i.i.d. data $X_1, \hdots, X_t$, the goal of the MLE scheme is to maximise,
\begin{equation}\label{eq:mle}
\hspace*{-1em}  \ell(\theta \, | \, x) \triangleq \ell(\theta_1, \hdots, \theta_n \, | \, x_1, \hdots, x_t) \triangleq \log \prod_{i=1}^t f(x_i \, | \, \theta_1, \hdots, \theta_n).
\end{equation}
$\ell(\cdot)$ is called the log-likelihood function. The set of parameters maximising Equation~\ref{eq:mle} is called the \emph{maximum likelihood estimator} of $\theta$ given the data $X_1, \hdots, X_t$. Maximum likelihood estimation has many desirable properties that we leverage in this work. For example, the ML estimator satisfies \emph{consistency}, i.e. under certain conditions, it achieves optimality even for \emph{constrained} MLE. An estimator being consistent means that if the data $X_1, \hdots, X_t$ is generated by $f(\cdot \, | \, \theta)$ and as $t\rightarrow \infty$, the estimate almost surely converges to the true parameter $\theta$. \citep{kiefer1956consistency} shows that MLE admits the consistency property given the following assumptions hold. The model is \emph{identifiable}, i.e. the densities at two parameter values must be different unless the two parameter values are identical. Further, the parameter space is \emph{compact} and \emph{continuous}. Finally, if the log-density is \emph{dominated}, one can establish that MLE converges to the true parameter almost surely~\citep{newey1987asymmetric}.
For problems where the likelihood is unbounded, flat or otherwise unstable one may introduce a penalty term in the objective function. This approach is called \emph{penalised maximum likelihood estimation}~\citep{ciuperca2003penalized, ouhamma2022bilinear}. As we in our work are mixing over known parameters, we do not need to add regularisation to our objective to guarantee convergence.

In this work, we iteratively collect data and compute new point estimates of the parameters and use them in our decision-making procedure. In order to carry out MLE, a likelihood function has to be chosen. In this work, we investigate two such likelihood functions in Section~\ref{sec:algorithms}, one for each respective model class. 